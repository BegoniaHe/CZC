/**
 * @file lexer_integration_test.cpp
 * @brief Lexer æ¨¡å—é›†æˆæµ‹è¯•ã€‚
 * @author BegoniaHe
 * @version 0.0.1
 * @date 2025-12-04
 *
 * @details
 *   æœ¬æ–‡ä»¶åŒ…å«è¯æ³•åˆ†æå™¨çš„é›†æˆæµ‹è¯•ï¼ŒéªŒè¯ï¼š
 *   - å®Œæ•´æºæ–‡ä»¶çš„è¯æ³•åˆ†æ
 *   - å¤šæ–‡ä»¶å¹¶å‘å¤„ç†
 *   - é”™è¯¯æ¢å¤å’Œè¯Šæ–­
 *   - ä¸ CLI å±‚çš„é›†æˆ
 */

#include "czc/cli/context.hpp"
#include "czc/cli/phases/lexer_phase.hpp"
#include "czc/lexer/lexer.hpp"

#include <filesystem>
#include <fstream>
#include <gtest/gtest.h>

namespace czc::lexer {
namespace {

class LexerIntegrationTest : public ::testing::Test {
protected:
  cli::CompilerContext ctx_;
  std::filesystem::path testDir_;

  void SetUp() override {
    // åˆ›å»ºä¸´æ—¶æµ‹è¯•ç›®å½•
    testDir_ = std::filesystem::temp_directory_path() / "czc_lexer_test";
    std::filesystem::create_directories(testDir_);
  }

  void TearDown() override {
    // æ¸…ç†ä¸´æ—¶æµ‹è¯•ç›®å½•
    std::filesystem::remove_all(testDir_);
  }

  /**
   * @brief åˆ›å»ºä¸´æ—¶æµ‹è¯•æ–‡ä»¶ã€‚
   */
  std::filesystem::path createTestFile(std::string_view filename,
                                        std::string_view content) {
    auto path = testDir_ / filename;
    std::ofstream ofs(path);
    ofs << content;
    return path;
  }
};

// ============================================================================
// å®Œæ•´æºæ–‡ä»¶æµ‹è¯•
// ============================================================================

TEST_F(LexerIntegrationTest, TokenizeCompleteSourceFile) {
  auto path = createTestFile("src.zero", R"(
// è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„æºæ–‡ä»¶ç¤ºä¾‹

fn add(a: i32, b: i32) -> i32 {
    return a + b;
}

fn main() {
    let x = 42;
    let y = 10;
    let result = add(x, y);
}
)");

  cli::LexerPhase phase(ctx_);
  auto result = phase.runOnFile(path);

  ASSERT_TRUE(result.has_value()) << "Lexer failed: "
                                   << result.error().message;
  EXPECT_FALSE(result->hasErrors);
  EXPECT_GT(result->tokens.size(), 20u);

  // éªŒè¯ç¬¬ä¸€ä¸ªæœ‰æ„ä¹‰çš„ Token æ˜¯ fn å…³é”®å­—
  // è·³è¿‡ TOKEN_COMMENT
  bool foundFn = false;
  for (const auto &token : result->tokens) {
    if (token.type() == TokenType::KW_FN) {
      foundFn = true;
      break;
    }
  }
  EXPECT_TRUE(foundFn) << "Expected 'fn' keyword in tokens";
}

TEST_F(LexerIntegrationTest, TokenizeWithTrivia) {
  ctx_.lexer().preserveTrivia = true;

  auto path = createTestFile("trivia.zero", R"(let x = 1; // comment
let y = 2;
)");

  cli::LexerPhase phase(ctx_);
  auto result = phase.runOnFile(path);

  ASSERT_TRUE(result.has_value());
  EXPECT_FALSE(result->hasErrors);

  // æ£€æŸ¥æ˜¯å¦æœ‰ Token å¸¦æœ‰ trivia
  bool hasLeadingTrivia = false;
  bool hasTrailingTrivia = false;
  for (const auto &token : result->tokens) {
    if (!token.leadingTrivia().empty()) {
      hasLeadingTrivia = true;
    }
    if (!token.trailingTrivia().empty()) {
      hasTrailingTrivia = true;
    }
  }

  EXPECT_TRUE(hasLeadingTrivia || hasTrailingTrivia)
      << "Expected trivia when preserveTrivia is enabled";
}

// ============================================================================
// é”™è¯¯å¤„ç†æµ‹è¯•
// ============================================================================

TEST_F(LexerIntegrationTest, HandleInvalidUtf8) {
  // åˆ›å»ºåŒ…å«æ— æ•ˆ UTF-8 åºåˆ—çš„æ–‡ä»¶
  auto path = testDir_ / "invalid_utf8.zero";
  std::ofstream ofs(path, std::ios::binary);
  ofs << "let x = \x80\x81\x82;"; // æ— æ•ˆçš„ UTF-8 åºåˆ—
  ofs.close();

  cli::LexerPhase phase(ctx_);
  auto result = phase.runOnFile(path);

  ASSERT_TRUE(result.has_value());
  // å³ä½¿æœ‰é”™è¯¯ï¼Œä¹Ÿåº”è¯¥ç”Ÿæˆ Tokenï¼ˆé”™è¯¯æ¢å¤ï¼‰
  EXPECT_GT(result->tokens.size(), 0u);
}

TEST_F(LexerIntegrationTest, HandleUnterminatedString) {
  auto path = createTestFile("unterminated.zero", R"(
let s = "unterminated string
let x = 1;
)");

  cli::LexerPhase phase(ctx_);
  auto result = phase.runOnFile(path);

  ASSERT_TRUE(result.has_value());
  EXPECT_TRUE(result->hasErrors);

  // å°½ç®¡æœ‰é”™è¯¯ï¼Œåç»­çš„ Token ä»åº”è¢«è§£æï¼ˆé”™è¯¯æ¢å¤ï¼‰
  bool foundLet = false;
  bool foundX = false;
  for (const auto &token : result->tokens) {
    if (token.type() == TokenType::KW_LET) {
      foundLet = true;
    }
    if (token.type() == TokenType::IDENTIFIER) {
      foundX = true;
    }
  }
  EXPECT_TRUE(foundLet) << "Error recovery should allow parsing subsequent tokens";
}

// ============================================================================
// å¤šæ–‡ä»¶å¤„ç†æµ‹è¯•
// ============================================================================

TEST_F(LexerIntegrationTest, ProcessMultipleFiles) {
  auto path1 = createTestFile("file1.zero", "let a = 1;");
  auto path2 = createTestFile("file2.zero", "let b = 2;");

  cli::LexerPhase phase1(ctx_);
  cli::LexerPhase phase2(ctx_);

  auto result1 = phase1.runOnFile(path1);
  auto result2 = phase2.runOnFile(path2);

  ASSERT_TRUE(result1.has_value());
  ASSERT_TRUE(result2.has_value());

  // éªŒè¯ä¸¤ä¸ªæ–‡ä»¶çš„ Token æ˜¯ç‹¬ç«‹çš„
  bool foundA = false;
  bool foundB = false;

  for (const auto &token : result1->tokens) {
    auto val = token.value(phase1.sourceManager());
    if (val == "a") foundA = true;
  }

  for (const auto &token : result2->tokens) {
    auto val = token.value(phase2.sourceManager());
    if (val == "b") foundB = true;
  }

  EXPECT_TRUE(foundA);
  EXPECT_TRUE(foundB);
}

// ============================================================================
// è¾¹ç•Œæ¡ä»¶æµ‹è¯•
// ============================================================================

TEST_F(LexerIntegrationTest, HandleEmptyFile) {
  auto path = createTestFile("empty.zero", "");

  cli::LexerPhase phase(ctx_);
  auto result = phase.runOnFile(path);

  ASSERT_TRUE(result.has_value());
  EXPECT_FALSE(result->hasErrors);
  ASSERT_EQ(result->tokens.size(), 1u);
  EXPECT_EQ(result->tokens[0].type(), TokenType::TOKEN_EOF);
}

TEST_F(LexerIntegrationTest, HandleWhitespaceOnlyFile) {
  auto path = createTestFile("whitespace.zero", "   \n\t\n   ");

  cli::LexerPhase phase(ctx_);
  auto result = phase.runOnFile(path);

  ASSERT_TRUE(result.has_value());
  EXPECT_FALSE(result->hasErrors);
  ASSERT_EQ(result->tokens.size(), 1u);
  EXPECT_EQ(result->tokens[0].type(), TokenType::TOKEN_EOF);
}

TEST_F(LexerIntegrationTest, HandleNonExistentFile) {
  std::filesystem::path nonExistent = testDir_ / "does_not_exist.zero";

  cli::LexerPhase phase(ctx_);
  auto result = phase.runOnFile(nonExistent);

  EXPECT_FALSE(result.has_value());
  EXPECT_EQ(result.error().code, "E001"); // File not found
}

// ============================================================================
// Unicode æ”¯æŒæµ‹è¯•
// ============================================================================

TEST_F(LexerIntegrationTest, HandleUnicodeIdentifiers) {
  auto path = createTestFile("unicode.zero", R"(
let å˜é‡ = 1;
let Î±Î²Î³ = 2;
let emojiğŸ‰ = 3;
)");

  cli::LexerPhase phase(ctx_);
  auto result = phase.runOnFile(path);

  ASSERT_TRUE(result.has_value());
  // æ ¹æ®è¯­è¨€è§„èŒƒï¼ŒæŸäº› Unicode å­—ç¬¦å¯èƒ½ä¸æ˜¯æœ‰æ•ˆçš„æ ‡è¯†ç¬¦
  // è¿™é‡Œä¸»è¦éªŒè¯ä¸ä¼šå´©æºƒ
}

TEST_F(LexerIntegrationTest, HandleUnicodeStrings) {
  auto path = createTestFile("unicode_strings.zero", R"(
let hello = "ä½ å¥½ä¸–ç•Œ";
let emoji = "ğŸ‰ğŸŠğŸ";
)");

  cli::LexerPhase phase(ctx_);
  auto result = phase.runOnFile(path);

  ASSERT_TRUE(result.has_value());
  EXPECT_FALSE(result->hasErrors);

  // éªŒè¯å­—ç¬¦ä¸²å­—é¢é‡è¢«æ­£ç¡®è§£æ
  int stringCount = 0;
  for (const auto &token : result->tokens) {
    if (token.type() == TokenType::LIT_STRING) {
      stringCount++;
    }
  }
  EXPECT_EQ(stringCount, 2);
}

} // namespace
} // namespace czc::lexer
